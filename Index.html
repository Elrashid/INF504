<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.14.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@6.7.0"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.14.4"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.14.4/dist/index.umd.min.js"></script><script>(()=>{setTimeout(()=>{const{markmap:Fe,mm:ur}=window,Fr=new Fe.Toolbar;Fr.attach(ur);const Dr=Fr.render();Dr.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(Dr)})})()</script><script>((B,S,M,_)=>{const I=B();window.mm=I.Markmap.create("svg#mindmap",(S||I.deriveOptions)(_),M)})(()=>window.markmap,null,{"type":"heading","depth":0,"payload":{"lines":[1,2]},"content":"Data Mining","children":[{"type":"heading","depth":1,"payload":{"lines":[5,6]},"content":"Week 1  Introduction to Data Mining","children":[{"type":"heading","depth":2,"payload":{"lines":[6,7]},"content":"Introduction to Data Mining","children":[{"type":"heading","depth":3,"payload":{"lines":[7,8]},"content":"Data mining is the non-trivial extraction of implicit, previously unknown, and potentially useful information from data."},{"type":"heading","depth":3,"payload":{"lines":[8,9]},"content":"It can be used to provide better, customized services for an edge in Customer Relationship Management or for scientific data collected and stored at enormous speeds."},{"type":"heading","depth":3,"payload":{"lines":[9,10]},"content":"Google digitized millions of documents from 1800 to 2000 and used OCR to count the occurrence of N-grams (sequence of words)."},{"type":"heading","depth":3,"payload":{"lines":[10,11]},"content":"This caused a revolution in research and publications, allowing for the first time studies about language evolution to be conducted in such scale in terms of time and documents."},{"type":"heading","depth":3,"payload":{"lines":[11,12]},"content":"The data mining process may also involve data integration."},{"type":"heading","depth":3,"payload":{"lines":[12,13]},"content":"It is not the same as data analysis or data reporting."}]},{"type":"heading","depth":2,"payload":{"lines":[14,15]},"content":"Data Mining Tasks and Applications","children":[{"type":"heading","depth":3,"payload":{"lines":[15,16]},"content":"Classification: Definition - Given a collection of records (training set ) Each record contains a set of attributes, one of the attributes is the class. Find a model for class attribute as a function of the values of other attributes. Goal: previously unseen records should be assigned a class as accurately as possible."},{"type":"heading","depth":3,"payload":{"lines":[16,17]},"content":"Classification: Application 1 - Direct Marketing - Reduce cost of mailing by targeting a set of consumers likely to buy a new cell-phone product."},{"type":"heading","depth":3,"payload":{"lines":[17,18]},"content":"Classification: Application 2 - Customer Attrition/Churn - Predict whether a customer is likely to be lost to a competitor."},{"type":"heading","depth":3,"payload":{"lines":[18,19]},"content":"Clustering [Descriptive] - Group together similar documents returned by search engine (e.g. Amazon rainforest, Amazon.com)."},{"type":"heading","depth":3,"payload":{"lines":[19,20]},"content":"Association Rule Discovery [Descriptive] - Look up phone number in phone directory."},{"type":"heading","depth":3,"payload":{"lines":[20,21]},"content":"Sequential Pattern Discovery [Descriptive] - Compute average age of BUiD students."},{"type":"heading","depth":3,"payload":{"lines":[21,22]},"content":"Regression [Predictive] - If you want to figure out this question, what data do you need? Is this a data mining?"},{"type":"heading","depth":3,"payload":{"lines":[22,23]},"content":"Deviation Detection [Predictive] - Group discussion: when to post on facebook/twitter? Does it make a difference when do you post?"},{"type":"heading","depth":3,"payload":{"lines":[23,24]},"content":"Origins of Data Mining - Draws ideas from machine learning/AI, pattern recognition, statistics, and database systems."},{"type":"heading","depth":3,"payload":{"lines":[24,25]},"content":"Traditional Techniques - may be unsuitable due to enormity of data, high dimensionality of data, heterogeneous, distributed nature of data."},{"type":"heading","depth":3,"payload":{"lines":[25,26]},"content":"Published in SIGKDD 2015 - When to post on social networks, can be found here: http://arxiv.org/pdf/1506.02089v1.pdf."},{"type":"heading","depth":3,"payload":{"lines":[26,27]},"content":"Two of my students conducted similar study in the UAE - Published a paper in major conference and working on another one about Ramadan!"},{"type":"heading","depth":3,"payload":{"lines":[27,28]},"content":"Some names more common in certain US locations (O’Brien, O’Rurke, O’Reilly… in Boston area)."}]},{"type":"heading","depth":2,"payload":{"lines":[29,30]},"content":"Applying Data Mining Techniques to Areej","children":[{"type":"heading","depth":3,"payload":{"lines":[30,31]},"content":"Areej can use data mining techniques to analyze and classify customer data, such as purchase patterns, age, and lifestyle information, to gain insights into customer behavior."},{"type":"heading","depth":3,"payload":{"lines":[31,32]},"content":"Clustering can be used to identify customer segments and determine which marketing mix is best suited for each segment."},{"type":"heading","depth":3,"payload":{"lines":[32,33]},"content":"Association rule discovery can be used to find relationships between different products, such as which products should be sold together to promote sales of a particular product."},{"type":"heading","depth":3,"payload":{"lines":[33,34]},"content":"Deviation/anomaly detection can be used to detect any significant deviations from normal customer behavior, which could be an indication of fraudulent activity."}]},{"type":"heading","depth":2,"payload":{"lines":[35,36]},"content":"SKICAT and Rhine Paradox","children":[{"type":"heading","depth":3,"payload":{"lines":[36,37]},"content":"SKICAT: Goal was to categorize astronomical objects using a model capable of classifying each detectable object into four predefined classes. Data set was 109 objects detectable in over 3 terabytes of high-resolution image data with 38 features measured. Search technique used was recursive building of the decision tree."},{"type":"heading","depth":3,"payload":{"lines":[37,38]},"content":"Rhine Paradox: Joseph Rhine was a parapsychologist in the 1950's who hypothesized that some people had Extra-Sensory Perception. Almost 1000 in 1000,000 had ESP in guessing 10 hidden cards. However, when he told these people they had ESP, almost all of them had lost their ESP. He concluded that you shouldn't tell people they have ESP; it causes them to lose it."}]},{"type":"heading","depth":2,"payload":{"lines":[39,40]},"content":"An Overview of Coursework and Teamwork Requirements","children":[{"type":"heading","depth":3,"payload":{"lines":[40,41]},"content":"Notes: We will study methods that ensure the discovered relationships are not by chance, thanks to Anand Rajaramanand Jeff Ullman."},{"type":"heading","depth":3,"payload":{"lines":[41,42]},"content":"Coursework Survey due week 5, 20% Project Report, due week 10, 30% Presentation, due week 10, 10%."},{"type":"heading","depth":3,"payload":{"lines":[42,43]},"content":"Form groups on blackboard and individual contribution will be assessed, with the description posted."},{"type":"heading","depth":3,"payload":{"lines":[43,44]},"content":"Use the draft submission on Turnitin, but it can only be used ONCE and will NOT be graded."},{"type":"heading","depth":3,"payload":{"lines":[44,45]},"content":"Teamwork: only one member needs to submit, and the first page should mention the group number and names."}]}]},{"type":"heading","depth":1,"payload":{"lines":[51,52]},"content":"Week 2-3 Data","children":[{"type":"heading","depth":2,"payload":{"lines":[53,54]},"content":"Preprocessing for Netflix 100+ Million Data Touples","children":[{"type":"heading","depth":3,"payload":{"lines":[54,55]},"content":"Normalization and scaling of numeric attributes may be needed to reduce the range of values and to bring all attributes to the same range."},{"type":"heading","depth":3,"payload":{"lines":[55,56]},"content":"Categorical attributes may need to be converted to numeric form using encoding techniques such as one-hot-encoding."},{"type":"heading","depth":3,"payload":{"lines":[56,57]},"content":"Feature extraction could be used to reduce the number of attributes by extracting the most important features."},{"type":"heading","depth":3,"payload":{"lines":[57,58]},"content":"Feature construction may be useful to combine existing features into a new feature that captures the most relevant information."},{"type":"heading","depth":3,"payload":{"lines":[58,59]},"content":"Discretization may be used to convert continuous attributes into discrete values."},{"type":"heading","depth":3,"payload":{"lines":[59,60]},"content":"Attribute transformation can be used to map entire set of values of an attribute to a new set of replacement values."},{"type":"heading","depth":3,"payload":{"lines":[60,61]},"content":"To reduce the data size, data sampling or dimensionality reduction techniques such as PCA, could be used."}]},{"type":"heading","depth":2,"payload":{"lines":[62,63]},"content":"Distance Metrics and Similarity Between Objects","children":[{"type":"heading","depth":3,"payload":{"lines":[63,64]},"content":"The most common distance metric is Euclidean distance, which assumes continuous attributes and can be standardized if necessary."},{"type":"heading","depth":3,"payload":{"lines":[64,65]},"content":"Minkowski distance is a generalization of Euclidean distance, and can take on different values for the parameter r."},{"type":"heading","depth":3,"payload":{"lines":[65,66]},"content":"Both distances and similarities have well-known properties such as positive definiteness, symmetry, and the triangle inequality."},{"type":"heading","depth":3,"payload":{"lines":[66,67]},"content":"When dealing with binary vectors, the Simple Matching and Jaccard coefficients are useful to calculate the similarity between objects."}]},{"type":"heading","depth":2,"payload":{"lines":[68,69]},"content":"Comparing Proximity Measures","children":[{"type":"heading","depth":3,"payload":{"lines":[69,70]},"content":"Cosine Similarity is used mostly for comparing text documents"},{"type":"heading","depth":3,"payload":{"lines":[70,71]},"content":"Correlation and Cosine are both invariant to scaling, while Euclidean Distance is not"},{"type":"heading","depth":3,"payload":{"lines":[71,72]},"content":"Extended Jaccard Coefficient (Tanimoto), Mahalanobis distance, and other proximity measures are also used"},{"type":"heading","depth":3,"payload":{"lines":[72,73]},"content":"Choice of the right proximity measure depends on the domain, e.g. documents, temperature, time series"}]},{"type":"heading","depth":2,"payload":{"lines":[74,75]},"content":"Measuring Proximity in Different Attribute Types","children":[{"type":"heading","depth":3,"payload":{"lines":[75,76]},"content":"Symmetry is a common property of proximity measures."},{"type":"heading","depth":3,"payload":{"lines":[76,77]},"content":"Tolerance to noise and outliers is another property that should be taken into account."},{"type":"heading","depth":3,"payload":{"lines":[77,78]},"content":"Ability to find more types of patterns should also be considered when measuring proximity."},{"type":"heading","depth":3,"payload":{"lines":[78,79]},"content":"Weights can be used to combine individual attribute distances."},{"type":"heading","depth":3,"payload":{"lines":[79,80]},"content":"Euclidean distance and Minkowsky dissimilarity can be used to measure distance."},{"type":"heading","depth":3,"payload":{"lines":[80,81]},"content":"SMC, Jaccard, and Cosine scores are used as similarity measures."},{"type":"heading","depth":3,"payload":{"lines":[81,82]},"content":"For Netflix dataset, similarity between two users can be measured by converting data into a table with users as rows and movies as columns."},{"type":"heading","depth":3,"payload":{"lines":[82,83]},"content":"To measure the similarity between two patients, distances between the attributes should be measured, and the results should be combined."}]}]},{"type":"heading","depth":1,"payload":{"lines":[96,97]},"content":"Week 3-4 Visualization","children":[{"type":"heading","depth":2,"payload":{"lines":[98,99]},"content":"Exploring Data Through Visualization","children":[{"type":"heading","depth":3,"payload":{"lines":[99,100]},"content":"Summary statistics, frequency, mode, percentiles, mean, median, range and variance are used to explore a given data set."},{"type":"heading","depth":3,"payload":{"lines":[100,101]},"content":"Visualization techniques are used to better understand data characteristics and help select the right tool for preprocessing or analysis."},{"type":"heading","depth":3,"payload":{"lines":[101,102]},"content":"Clustering and anomaly detection are also used as exploratory techniques."},{"type":"heading","depth":3,"payload":{"lines":[102,103]},"content":"Frequency of an attribute value is the percentage of time the value occurs in data."},{"type":"heading","depth":3,"payload":{"lines":[103,104]},"content":"Percentiles are used with continuous data and the pth percentile is a value such that p% of the observed values of x are less than."},{"type":"heading","depth":3,"payload":{"lines":[104,105]},"content":"Mean and median are measures of the location of a set of points, while range and variance are measures of spread."},{"type":"heading","depth":3,"payload":{"lines":[105,106]},"content":"Motivating example of data representing diseases in different emirates is used to demonstrate how visualization can be used."}]},{"type":"heading","depth":2,"payload":{"lines":[107,108]},"content":"Exploring Data Visualization","children":[{"type":"heading","depth":3,"payload":{"lines":[108,109]},"content":"Visualizing data can be done manually or through computers, depending on the task and the amount of data available."},{"type":"heading","depth":3,"payload":{"lines":[109,110]},"content":"Human-in-the-loop visualizations can be used to explore unknown datasets and present known results."},{"type":"heading","depth":3,"payload":{"lines":[110,111]},"content":"Computer-in-the-loop visualizations are used to explore large datasets and observe changes over time."},{"type":"heading","depth":3,"payload":{"lines":[111,112]},"content":"Vision is used to provide a high-bandwidth channel to the brain, allowing for an overview of the data."},{"type":"heading","depth":3,"payload":{"lines":[112,113]},"content":"Representing data in detail is important as summaries can lose information, and details can help to confirm expected and find unexpected patterns."},{"type":"heading","depth":3,"payload":{"lines":[113,114]},"content":"Resource limitations, such as bandwidth and storage, need to be taken into account when exploring data visualization."}]},{"type":"heading","depth":2,"payload":{"lines":[115,116]},"content":"Visualization: Exploring Marks, Channels, and Idioms","children":[{"type":"heading","depth":3,"payload":{"lines":[116,117]},"content":"Marks represent geometric primitives and control the appearance of channels."},{"type":"heading","depth":3,"payload":{"lines":[117,118]},"content":"Channels can be used to redundantly code information and can include vertical and horizontal positions, color hues, and size (area)."},{"type":"heading","depth":3,"payload":{"lines":[118,119]},"content":"A mark type for table data set is a point and for network data set, two marks are used, one for nodes and another for links."},{"type":"heading","depth":3,"payload":{"lines":[119,120]},"content":"The expressiveness principle is to match the channel and data characteristics, while the effectiveness principle is to encode the most important attributes with the highest ranked channels."},{"type":"heading","depth":3,"payload":{"lines":[120,121]},"content":"Spatial position ranks high for both relative luminance and color judgements."},{"type":"heading","depth":3,"payload":{"lines":[121,122]},"content":"Common visualization idioms are scatterplots and bar charts."},{"type":"heading","depth":3,"payload":{"lines":[122,123]},"content":"Scatterplots express values for quantitative attributes, while bar charts compare values for one categorical attribute and one quantitative attribute."},{"type":"heading","depth":3,"payload":{"lines":[123,124]},"content":"Limitations of bar charts include difficulty in knowing the rank between items."}]},{"type":"heading","depth":2,"payload":{"lines":[125,126]},"content":"Visualizing Data with Idioms","children":[{"type":"heading","depth":3,"payload":{"lines":[126,127]},"content":"Idiom: line chart / dot plot - one key, one value data with two quantitative attributes, mark with points and a line connection to show the relationship between them, and channels aligned lengths to express quantitative value, separated and ordered by key attribute into horizontal regions."},{"type":"heading","depth":3,"payload":{"lines":[127,128]},"content":"Idiom: bar vs line charts - depending on the type of key attribute, bar charts are used when categorical, and line charts when ordered. Do not use line charts for categorical key attributes as it violates the expressiveness principle."},{"type":"heading","depth":3,"payload":{"lines":[128,129]},"content":"Idiom: dual-axis line charts - two values data with two quantitative value attributes (one derived attribute: change magnitude), mark with point and line, line connecting the mark between the points, and channels two vertical position to express attribute value (line width/size, color)."},{"type":"heading","depth":3,"payload":{"lines":[129,130]},"content":"Idiom: slopegraphs - two values data with two quantitative value attributes, mark with point and line, line connecting the mark between the points, and channels two vertical position to express attribute value (line width/size, color). Task is to emphasize changes in rank/value."},{"type":"heading","depth":3,"payload":{"lines":[130,131]},"content":"Idiom: connected scatterplots - scatterplot with line connection marks, horizontal and vertical axes for value attributes, line connection marks for temporal order, and alternative to dual-axis charts."},{"type":"heading","depth":3,"payload":{"lines":[131,132]},"content":"Idiom: Gantt charts - one key, two (related) values data with one categorical attribute and two quantitative attributes, mark with line length for duration, channels horizontal position for start time and end from duration, task is to emphasize temporal overlaps and start/end dependencies between items."},{"type":"heading","depth":3,"payload":{"lines":[132,133]},"content":"Idiom: scatterplot matrix, parallel coordinates - scatterplot matrix with rectilinear axes and point mark for all possible pairs of axes, parallel coordinates with parallel axes and a jagged line representing an item, rectilinear axes and item as point, and axis ordering is a major challenge."}]}]},{"type":"heading","depth":1,"payload":{"lines":[148,149]},"content":"Week 5 Classification","children":[{"type":"heading","depth":2,"payload":{"lines":[152,153]},"content":"Introduction to Data Mining Classification: Basic Concepts, Decision Trees, and Model Evaluation","children":[{"type":"heading","depth":3,"payload":{"lines":[153,154]},"content":"Classification is the process of assigning a class to a given set of records based on other attributes."},{"type":"heading","depth":3,"payload":{"lines":[154,155]},"content":"Visualization techniques, such as scatter plots, parallel coordinates, and histograms, are used to map data to visual cues."},{"type":"heading","depth":3,"payload":{"lines":[155,156]},"content":"Decision tree based methods and instance based methods are two types of classification techniques."},{"type":"heading","depth":3,"payload":{"lines":[156,157]},"content":"The Hunt’s Algorithm is an example of a decision tree induction algorithm."},{"type":"heading","depth":3,"payload":{"lines":[157,158]},"content":"The model evaluation process involves testing the accuracy of the model on a test set."}]},{"type":"heading","depth":2,"payload":{"lines":[159,160]},"content":"Tree Induction: Split the Records Based on Attribute Test","children":[{"type":"heading","depth":3,"payload":{"lines":[160,161]},"content":"Splitting Based on Nominal Attributes: Multi-way split, which uses as many partitions as distinct values, or binary split, which divides values into two subsets"},{"type":"heading","depth":3,"payload":{"lines":[161,162]},"content":"Splitting Based on Ordinal Attributes: Multi-way split or binary split"},{"type":"heading","depth":3,"payload":{"lines":[162,163]},"content":"Splitting Based on Continuous Attributes: Discretization, static or dynamic ranges, binary decision, finding the best cut"},{"type":"heading","depth":3,"payload":{"lines":[163,164]},"content":"How to determine the best split: Greedy approach, looking at records before splitting"}]},{"type":"heading","depth":2,"payload":{"lines":[165,166]},"content":"Understanding Node Impurity, Tree Induction and Decision Tree Based Classification","children":[{"type":"heading","depth":3,"payload":{"lines":[166,167]},"content":"Measures of Node Impurity: Gini Index and Entropy are used to measure the degree of homogeneity of a node. Maximum impurity is 1-1/nc when records are equally distributed among all classes, and minimum impurity is 0 when all records belong to one class."},{"type":"heading","depth":3,"payload":{"lines":[167,168]},"content":"Tree Induction: A greedy strategy is used to split records based on an attribute test that optimizes a certain criterion. Issues such as determining how to split the records, how to specify the attribute test condition, how to determine the best split and when to stop splitting must be considered."},{"type":"heading","depth":3,"payload":{"lines":[168,169]},"content":"Stopping Criteria for Tree Induction: Stop expanding a node when all records belong to the same class, and stop expanding a node when all records have similar attribute values. Early termination (pre-pruning) is also possible."},{"type":"heading","depth":3,"payload":{"lines":[169,170]},"content":"Decision Boundary: A border line between two neighboring regions of different classes is known as a decision boundary. Decision boundaries are parallel to axes as the test condition involves a single attribute at-a-time."},{"type":"heading","depth":3,"payload":{"lines":[170,171]},"content":"Decision Tree Based Classification: Advantages include being inexpensive to construct and fast at classifying unknown records, while disadvantages include not being efficient with continuous attributes."},{"type":"heading","depth":3,"payload":{"lines":[171,172]},"content":"Exercise: To determine the tree corresponding to the decision boundary."},{"type":"heading","depth":3,"payload":{"lines":[172,173]},"content":"Other Classifiers: K-nearest neighbor, Neural networks and Instance-Based Classifiers are other classification techniques."},{"type":"heading","depth":3,"payload":{"lines":[173,174]},"content":"Nearest Neighbor Classifiers: Require a set of stored records, a similarity/dissimilarity measure, and a value for k (number of nearest neighbors to retrieve). Class labels of nearest neighbors are used to determine the class label of the unknown record."}]},{"type":"heading","depth":2,"payload":{"lines":[175,176]},"content":"Overfitting and Underfitting in Classification","children":[{"type":"heading","depth":3,"payload":{"lines":[176,177]},"content":"Overfitting occurs when a model is too complex and fits the training data “too” well. This results in models (e.g., decision trees) that are more complex than necessary."},{"type":"heading","depth":3,"payload":{"lines":[177,178]},"content":"Underfitting occurs when the model is too simple, causing both training and test errors to be large."},{"type":"heading","depth":3,"payload":{"lines":[178,179]},"content":"To detect overfitting, it is necessary to have better ways to estimate errors."},{"type":"heading","depth":3,"payload":{"lines":[179,180]},"content":"Data preparation is a practical issue to keep in mind when doing classification."},{"type":"heading","depth":3,"payload":{"lines":[180,181]},"content":"Costs of classification must also be taken into account."}]},{"type":"heading","depth":2,"payload":{"lines":[182,183]},"content":"Evaluating Model Performance","children":[{"type":"heading","depth":3,"payload":{"lines":[183,184]},"content":"Model evaluation should focus on the predictive capability of a model, not the speed of classification or scalability."},{"type":"heading","depth":3,"payload":{"lines":[184,185]},"content":"Confusion Matrix is a widely-used metric for model evaluation, which looks at True Positives, False Negatives, False Positives, and True Negatives."},{"type":"heading","depth":3,"payload":{"lines":[185,186]},"content":"Accuracy can be misleading, so cost-sensitive measures such as precision, recall, and F-measure should be considered."},{"type":"heading","depth":3,"payload":{"lines":[186,187]},"content":"Methods of estimation include holdout, cross validation, and leave-one-out."}]}]},{"type":"heading","depth":1,"payload":{"lines":[201,202]},"content":"Week 6 Clustering","children":[{"type":"heading","depth":2,"payload":{"lines":[203,204]},"content":"K-means and Hierarchical Clustering","children":[{"type":"heading","depth":3,"payload":{"lines":[204,205]},"content":"K-means and Hierarchical Clustering are two types of cluster analysis used to find groups of objects that are similar or related."},{"type":"heading","depth":3,"payload":{"lines":[205,206]},"content":"Partitional clustering divides data objects into non-overlapping subsets (clusters) such that each data object is in exactly one subset."},{"type":"heading","depth":3,"payload":{"lines":[206,207]},"content":"Hierarchical clustering produces a set of nested clusters organized as a hierarchical tree."},{"type":"heading","depth":3,"payload":{"lines":[207,208]},"content":"K-means clustering is a partitional clustering approach which assigns points to clusters based on the closest centroid."},{"type":"heading","depth":3,"payload":{"lines":[208,209]},"content":"The closeness is typically measured by Euclidean distance, cosine similarity, correlation, etc."},{"type":"heading","depth":3,"payload":{"lines":[209,210]},"content":"Convergence usually happens in the first few iterations and the time complexity is O(n<em>K</em>I*d)."},{"type":"heading","depth":3,"payload":{"lines":[210,211]},"content":"Non-traditional hierarchical clustering uses a non-traditional dendrogram."}]},{"type":"heading","depth":2,"payload":{"lines":[212,213]},"content":"Evaluating Different K-means Clusterings","children":[{"type":"heading","depth":3,"payload":{"lines":[213,214]},"content":"K-means is not always optimal, as different clusterings can be found for the same data and K."},{"type":"heading","depth":3,"payload":{"lines":[214,215]},"content":"The importance of choosing initial centroids is highlighted by examples."},{"type":"heading","depth":3,"payload":{"lines":[215,216]},"content":"The most common measure used to evaluate clusters is the Sum of Squared Error (SSE)."},{"type":"heading","depth":3,"payload":{"lines":[216,217]},"content":"Multiple runs of the K-means algorithm can help, but the probability of getting the right answer is small."},{"type":"heading","depth":3,"payload":{"lines":[217,218]},"content":"Hierarchical clustering can be used to determine initial centroids."},{"type":"heading","depth":3,"payload":{"lines":[218,219]},"content":"Post-processing and Bisecting K-means can help mitigate initialization issues."},{"type":"heading","depth":3,"payload":{"lines":[219,220]},"content":"If empty clusters appear, strategies such as choosing the point that contributes most to the SSE or a point from the cluster with the highest SSE can be employed."},{"type":"heading","depth":3,"payload":{"lines":[220,221]},"content":"Incrementally updating centroids is an alternative to the basic K-means algorithm."}]},{"type":"heading","depth":2,"payload":{"lines":[222,223]},"content":"Clustering Algorithms: K-Means, Hierarchical Clustering, and Density-Based Clustering","children":[{"type":"heading","depth":3,"payload":{"lines":[223,224]},"content":"K-means is a clustering algorithm that can produce a partitional or a hierarchical clustering. Its limitations include clusters of different sizes, densities, and non-globular shapes, as well as outliers."},{"type":"heading","depth":3,"payload":{"lines":[224,225]},"content":"Hierarchical clustering produces a set of nested clusters organized as a hierarchical tree, and can be visualized as a dendrogram. It does not require any particular number of clusters and can correspond to meaningful taxonomies."},{"type":"heading","depth":3,"payload":{"lines":[225,226]},"content":"There are two types of hierarchical clustering: agglomerative and divisive. Agglomerative clustering starts with the points as individual clusters and merges the closest pair of clusters until only one cluster (or k clusters) is left. Divisive clustering starts with one, all-inclusive cluster and splits a cluster until each cluster contains a point (or there are k clusters)."},{"type":"heading","depth":3,"payload":{"lines":[226,227]},"content":"Density-based clustering is based on the concept of connected components, defined by a given distance. It identifies clusters of arbitrary shapes."}]},{"type":"heading","depth":2,"payload":{"lines":[228,229]},"content":"Understanding Cluster Similarity","children":[{"type":"heading","depth":3,"payload":{"lines":[229,230]},"content":"MIN (Minimum) is a method of determining the similarity of two clusters based on the two most similar (closest) points in the different clusters."},{"type":"heading","depth":3,"payload":{"lines":[230,231]},"content":"Its strength lies in its ability to handle non-elliptical shapes."},{"type":"heading","depth":3,"payload":{"lines":[231,232]},"content":"It is determined by one pair of points, i.e., by one link in the proximity graph."},{"type":"heading","depth":3,"payload":{"lines":[232,233]},"content":"MAX (Maximum) is another method that uses the two most dissimilar (farthest) points in the different clusters."},{"type":"heading","depth":3,"payload":{"lines":[233,234]},"content":"Group Average determines the similarity of two clusters by calculating the average distance of all points in the two clusters."},{"type":"heading","depth":3,"payload":{"lines":[234,235]},"content":"Distance between Centroids is the distance between the centroids of the two clusters."},{"type":"heading","depth":3,"payload":{"lines":[235,236]},"content":"Other methods use an objective function to determine the similarity of two clusters, such as Ward’s Method which uses the squared error."}]},{"type":"heading","depth":2,"payload":{"lines":[237,238]},"content":"Comparing Hierarchical Clustering Algorithms","children":[{"type":"heading","depth":3,"payload":{"lines":[238,239]},"content":"MAX: Less susceptible to noise and outliers, but tends to break larger clusters and is biased towards globular clusters"},{"type":"heading","depth":3,"payload":{"lines":[239,240]},"content":"Group Average: Compromise between single and complete link, less susceptible to noise and outliers, but biased towards globular clusters"},{"type":"heading","depth":3,"payload":{"lines":[240,241]},"content":"Ward's Method: Less susceptible to noise and outliers, biased towards globular clusters, hierarchical analogue of K-means"},{"type":"heading","depth":3,"payload":{"lines":[241,242]},"content":"Time/Space requirements: O(N2) space, O(N3) time in many cases, complexity can be reduced to O(N2 log(N)) time for some approaches"},{"type":"heading","depth":3,"payload":{"lines":[242,243]},"content":"Problems &amp; Limitations: Sensitivity to noise and outliers, difficulty handling different sized clusters and convex shapes, breaking large clusters"}]},{"type":"heading","depth":2,"payload":{"lines":[244,245]},"content":"Cluster Validity: External, Internal and Relative Indices","children":[{"type":"heading","depth":3,"payload":{"lines":[245,246]},"content":"External Index: Used to measure the extent to which cluster labels match externally supplied class labels."},{"type":"heading","depth":3,"payload":{"lines":[246,247]},"content":"Internal Index: Used to measure the goodness of a clustering structure without respect to external information."},{"type":"heading","depth":3,"payload":{"lines":[247,248]},"content":"Relative Index: Used to compare two different clusterings or clusters."},{"type":"heading","depth":3,"payload":{"lines":[248,249]},"content":"Correlation of incidence and proximity matrices for K-means clusterings to measure cluster validity."},{"type":"heading","depth":3,"payload":{"lines":[249,250]},"content":"Entropy and Sum of Squared Error (SSE) are also used to measure cluster validity."},{"type":"heading","depth":3,"payload":{"lines":[250,251]},"content":"Criterion vs Index: Criterion is the general strategy and index is the numerical measure that implements the criterion."}]},{"type":"heading","depth":2,"payload":{"lines":[252,253]},"content":"Using Similarity Matrix and Internal Measures for Cluster Validation","children":[{"type":"heading","depth":3,"payload":{"lines":[253,254]},"content":"Corr = -0.9235 and Corr = -0.5810 are used to order the similarity matrix with respect to cluster labels and inspect visually."},{"type":"heading","depth":3,"payload":{"lines":[254,255]},"content":"DBSCAN, K-means, and Complete Link are used to identify clusters in random data which are not so crisp."},{"type":"heading","depth":3,"payload":{"lines":[255,256]},"content":"DBSCAN is not a good measure for some density or contiguity based clusters."},{"type":"heading","depth":3,"payload":{"lines":[256,257]},"content":"SSE is used as an internal index to measure the goodness of a clustering structure without respect to external information."},{"type":"heading","depth":3,"payload":{"lines":[257,258]},"content":"SSE is good for comparing two clusterings or two clusters and can also be used to estimate the number of clusters."},{"type":"heading","depth":3,"payload":{"lines":[258,259]},"content":"A framework is needed to interpret any measure for cluster validity, and to compare the results of two different sets of cluster analyses."}]}]},{"type":"heading","depth":1,"payload":{"lines":[270,271]},"content":"Week 7 Association rules and Retrieval by Content","children":[{"type":"heading","depth":2,"payload":{"lines":[272,273]},"content":"Association rules","children":[{"type":"heading","depth":3,"payload":{"lines":[274,275]},"content":"Discovering Association Rules","children":[{"type":"heading","depth":4,"payload":{"lines":[275,276]},"content":"Patterns and rules can be used to recommend movies to users using the Netflix data set."},{"type":"heading","depth":4,"payload":{"lines":[276,277]},"content":"A primitive pattern is an interesting information about part of the data and a model applies to the whole data."},{"type":"heading","depth":4,"payload":{"lines":[277,278]},"content":"Patterns can be combined using AND () and OR ()."},{"type":"heading","depth":4,"payload":{"lines":[278,279]},"content":"Market basket patterns are used in chain stores to understand customer behaviour."},{"type":"heading","depth":4,"payload":{"lines":[279,280]},"content":"Mining text, genetics and other many-many relationships can be used to discover patterns."},{"type":"heading","depth":4,"payload":{"lines":[280,281]},"content":"Carrefour sells 100,000 items and the Web has over 100,000,000 words."}]},{"type":"heading","depth":3,"payload":{"lines":[282,283]},"content":"Finding Frequent Itemsets and Association Rules","children":[{"type":"heading","depth":4,"payload":{"lines":[283,284]},"content":"Finding frequent itemsets and association rules involves an two-step approach of first generating all itemsets whose support is greater than or equal to a given support threshold, then generating rules with confidence greater than a given confidence threshold from each frequent itemset."},{"type":"heading","depth":4,"payload":{"lines":[284,285]},"content":"Previous classification techniques largely used machine learning and were not concerned with accessing hard-disk. In this problem, we need to optimize disk access."},{"type":"heading","depth":4,"payload":{"lines":[285,286]},"content":"Frequency of an itemset is the number of records where the set is true, and support for the itemset is the frequency divided by the total number of baskets/records."},{"type":"heading","depth":4,"payload":{"lines":[286,287]},"content":"An association rule is an &quot;if-then&quot; rule about the contents of the baskets, and the confidence of the rule is the probability of the item given the set of items mentioned."},{"type":"heading","depth":4,"payload":{"lines":[287,288]},"content":"Data is typically kept in a flat file and stored basket-by-basket on disk."},{"type":"heading","depth":4,"payload":{"lines":[288,289]},"content":"The true cost of mining disk-resident data is usually the number of disk I/O’s."}]},{"type":"heading","depth":3,"payload":{"lines":[290,291]},"content":"Apriori Algorithm","children":[{"type":"heading","depth":4,"payload":{"lines":[291,292]},"content":"The Apriori Algorithm is used to find frequent itemsets in a database."},{"type":"heading","depth":4,"payload":{"lines":[292,293]},"content":"Pass 1 of the algorithm requires reading baskets and counting in main memory the occurrences of each item."},{"type":"heading","depth":4,"payload":{"lines":[293,294]},"content":"Pass 2 reads baskets again and counts in main memory only those pairs both of which were found in Pass 1 to be frequent."},{"type":"heading","depth":4,"payload":{"lines":[294,295]},"content":"The Apriori algorithm works by merging any two itemsets in Lk that have first k-1 items in common and removing any new item with any subset of size k not in Lk."},{"type":"heading","depth":4,"payload":{"lines":[295,296]},"content":"The algorithm requires a minimum support value to determine which itemsets are considered frequent."}]},{"type":"heading","depth":3,"payload":{"lines":[297,298]},"content":"A-priori Algorithm &amp; Constructing Rules","children":[{"type":"heading","depth":4,"payload":{"lines":[298,299]},"content":"A-priori algorithm needs k passes over the database, where k is the number of items in the largest frequent itemset."},{"type":"heading","depth":4,"payload":{"lines":[299,300]},"content":"For each frequent itemset, generate all possible splits into two subsets and compute confidence for each rule."},{"type":"heading","depth":4,"payload":{"lines":[300,301]},"content":"To find only “interesting” rules, use the J-measure or the interest measure."},{"type":"heading","depth":4,"payload":{"lines":[301,302]},"content":"Rules with high support and confidence may be useful even if they are not “interesting”."}]}]},{"type":"heading","depth":2,"payload":{"lines":[303,304]},"content":"Building a System for Recommending Movies to Users Using A-priori Algorithm","children":[{"type":"heading","depth":3,"payload":{"lines":[304,305]},"content":"Pre-processing of the Netflix dataset is required, which includes User_id, movie_id, date, and rating."},{"type":"heading","depth":3,"payload":{"lines":[305,306]},"content":"A-priori algorithm can be used to build a system for recommending movies to users."},{"type":"heading","depth":3,"payload":{"lines":[306,307]},"content":"The algorithm uses three parameters: frequency, support, and confidence."},{"type":"heading","depth":3,"payload":{"lines":[307,308]},"content":"Frequency is the number of times an itemset appears in the dataset."},{"type":"heading","depth":3,"payload":{"lines":[308,309]},"content":"Support is the proportion of the dataset where the itemset appears."},{"type":"heading","depth":3,"payload":{"lines":[309,310]},"content":"Confidence is the probability that an item in the itemset will be bought."},{"type":"heading","depth":3,"payload":{"lines":[310,311]},"content":"To apply A-priori algorithm, the dataset should be examined for patterns and rules."},{"type":"heading","depth":3,"payload":{"lines":[311,312]},"content":"Group discussion helps to analyze the dataset and identify patterns."},{"type":"heading","depth":3,"payload":{"lines":[312,313]},"content":"Slide 47 can be used to explain the concepts of A-priori algorithm."}]},{"type":"heading","depth":2,"payload":{"lines":[314,315]},"content":"Retrieval by Content","children":[{"type":"heading","depth":3,"payload":{"lines":[315,316]},"content":"Text Retrieval","children":[{"type":"heading","depth":4,"payload":{"lines":[316,317]},"content":"Representing text involves Natural Language Processing (NLP) techniques and bag-of-words"},{"type":"heading","depth":4,"payload":{"lines":[317,318]},"content":"Vector Space Representation assigns weights to terms in the text to compute the distance between a document and query"},{"type":"heading","depth":4,"payload":{"lines":[318,319]},"content":"Euclidean and Cosine distance metric are used, with Cosine being more effective in practice"},{"type":"heading","depth":4,"payload":{"lines":[319,320]},"content":"Not all terms are equally important; terms like 'great' and 'very' have lower importance"}]},{"type":"heading","depth":3,"payload":{"lines":[321,322]},"content":"Text Retrieval and Word Representation","children":[{"type":"heading","depth":4,"payload":{"lines":[322,323]},"content":"TF-IDF is a method used to boost rare terms and avoid frequency bias in text retrieval."},{"type":"heading","depth":4,"payload":{"lines":[323,324]},"content":"Word embedding is an advanced text representation method, which uses neural networks to represent each word in a vocabulary as a point in a vector space."},{"type":"heading","depth":4,"payload":{"lines":[324,325]},"content":"Cosine distance is used to rank documents and avoid document size bias."},{"type":"heading","depth":4,"payload":{"lines":[325,326]},"content":"Word2Vec and GloVe are known techniques used for word embedding."}]}]}]},{"type":"heading","depth":1,"payload":{"lines":[330,331]},"content":"Week 8 Graph Mining &amp; Time series 1","children":[{"type":"heading","depth":2,"payload":{"lines":[332,333]},"content":"Graph Mining","children":[{"type":"heading","depth":3,"payload":{"lines":[334,335]},"content":"Graph Mining and Network Analysis","children":[{"type":"heading","depth":4,"payload":{"lines":[335,336]},"content":"Graphs and networks are important data structures used in data mining and analysis."},{"type":"heading","depth":4,"payload":{"lines":[336,337]},"content":"Graphs represent physical and cyber networks, such as the Internet, social networks, cellular networks in biology, food webs, and neural networks."},{"type":"heading","depth":4,"payload":{"lines":[337,338]},"content":"Graph mining includes tasks such as visualization, pattern discovery, classification, and descriptive/generative modeling."},{"type":"heading","depth":4,"payload":{"lines":[338,339]},"content":"Graphs are composed of nodes and edges and can be directed or undirected, weighted or unweighted."},{"type":"heading","depth":4,"payload":{"lines":[339,340]},"content":"Degree distribution is an important property of graphs and is used to summarize information and extract important features."},{"type":"heading","depth":4,"payload":{"lines":[340,341]},"content":"Graphs are used to detect communities and to analyze trust and importance in network exchanges."}]},{"type":"heading","depth":3,"payload":{"lines":[342,343]},"content":"Degree Distribution &amp; Power Law","children":[{"type":"heading","depth":4,"payload":{"lines":[343,344]},"content":"Degree distribution is the histogram of degrees and is represented by a plot of how many nodes have different degree values."},{"type":"heading","depth":4,"payload":{"lines":[344,345]},"content":"Frequent subgraphs are found by extending association rule mining and can be useful in web mining, computational chemistry, bioinformatics, and spatial data sets."},{"type":"heading","depth":4,"payload":{"lines":[345,346]},"content":"Apriori-like algorithm is used to find frequent subgraphs and support counting is used to count the support of each remaining candidate."},{"type":"heading","depth":4,"payload":{"lines":[346,347]},"content":"Degree distribution and power law are used to look at author-author connections and compute the degree distribution for different fields."},{"type":"heading","depth":4,"payload":{"lines":[347,348]},"content":"Diversity can have a significant impact on number of citations."},{"type":"heading","depth":4,"payload":{"lines":[348,349]},"content":"Diameter is the longest shortest path, and prior work indicates it slowly grows in power law graphs."},{"type":"heading","depth":4,"payload":{"lines":[349,350]},"content":"Faloutsos examined diameter of ArXiv citation graph and affiliation network in physics from 1992 to 2003."}]}]},{"type":"heading","depth":2,"payload":{"lines":[352,353]},"content":"Time series","children":[{"type":"heading","depth":3,"payload":{"lines":[354,355]},"content":"Time Series Data Mining","children":[{"type":"heading","depth":4,"payload":{"lines":[355,356]},"content":"Time series data is a collection of observations made sequentially in time and is found in many medical, scientific and business domains."},{"type":"heading","depth":4,"payload":{"lines":[356,357]},"content":"Working with time series data is difficult as it requires efficient storage and manipulation, as well as subjective similarity matching."},{"type":"heading","depth":4,"payload":{"lines":[357,358]},"content":"Common tasks with time series data include clustering, classification, query by content, rule discovery, motif discovery, novelty detection, and visualization."},{"type":"heading","depth":4,"payload":{"lines":[358,359]},"content":"An example of a time series data application is a doctor searching for similar ECG patterns to aid in diagnosis."}]},{"type":"heading","depth":3,"payload":{"lines":[360,361]},"content":"Preprocessing Data to Calculate Euclidean and Dynamic Time Warping Distance","children":[{"type":"heading","depth":4,"payload":{"lines":[361,362]},"content":"Offset Translation: subtract mean of two time series"},{"type":"heading","depth":4,"payload":{"lines":[362,363]},"content":"Amplitude Scaling: subtract mean and divide by standard deviation"},{"type":"heading","depth":4,"payload":{"lines":[363,364]},"content":"Noise Reduction: average each datapoints value with its neighbors"},{"type":"heading","depth":4,"payload":{"lines":[364,365]},"content":"Fixed Time Axis: align sequences one-to-one"},{"type":"heading","depth":4,"payload":{"lines":[365,366]},"content":"Warped Time Axis: allow for nonlinear alignments"},{"type":"heading","depth":4,"payload":{"lines":[366,367]},"content":"Euclidean Distance: subtract mean, divide by standard deviation, then calculate distance"},{"type":"heading","depth":4,"payload":{"lines":[367,368]},"content":"Dynamic Time Warping: align sequences with nonlinear alignments and calculate distance"}]},{"type":"heading","depth":3,"payload":{"lines":[369,370]},"content":"Fast Approximations to Dynamic Time Warp Distance","children":[{"type":"heading","depth":4,"payload":{"lines":[370,371]},"content":"We can approximate time series with a compressed or downsampled representation, and do DTW on the new representation."},{"type":"heading","depth":4,"payload":{"lines":[371,372]},"content":"We calculated the Euclidean Distance and DTW on a series of problems to compare the two."},{"type":"heading","depth":4,"payload":{"lines":[372,373]},"content":"We then visualized the cumulative matrix on a real world problem to observe the difference."},{"type":"heading","depth":4,"payload":{"lines":[373,374]},"content":"We saw that DTW produces much better results than Euclidean distance, but is very slow to calculate."},{"type":"heading","depth":4,"payload":{"lines":[374,375]},"content":"We explored the idea of fast approximations to Dynamic Time Warp Distance to speed up similarity search."}]},{"type":"heading","depth":3,"payload":{"lines":[376,377]},"content":"Lower Bounding for Dynamic Time Warp Distance","children":[{"type":"heading","depth":4,"payload":{"lines":[377,378]},"content":"Lower bounding is a technique used to speed up a single DTW calculation, as it is often the case when dealing with massive time series."},{"type":"heading","depth":4,"payload":{"lines":[378,379]},"content":"Algorithm 1 is used to find the closest sequence Ci to Q, by going through all example sequences to find the best."},{"type":"heading","depth":4,"payload":{"lines":[379,380]},"content":"Lower Bounding I involves two functions: DTW(A,B) and lower_bound_distance(A,B); the latter is faster, and by definition, lower_bound_distance(A,B) ≤ DTW(A,B)."},{"type":"heading","depth":4,"payload":{"lines":[380,381]},"content":"Algorithm II uses the lower bound distance to speed up the process by only doing the expensive calculations when absolutely necessary."},{"type":"heading","depth":4,"payload":{"lines":[381,382]},"content":"To lower bound DTW, researchers have found good approximations such as LB of Keogh. For large datasets, using lower-bounding, DTW is not significantly more expensive than Euclidean."},{"type":"heading","depth":4,"payload":{"lines":[382,383]},"content":"Lower bound of Yi is also discussed in the text."}]},{"type":"heading","depth":3,"payload":{"lines":[384,385]},"content":"Lower Bounding Measure for Time Warping","children":[{"type":"heading","depth":4,"payload":{"lines":[385,386]},"content":"Lower bounding measure can be used to represent the minimum points contribution to the overall DTW distance."},{"type":"heading","depth":4,"payload":{"lines":[386,387]},"content":"Yi, B, Jagadish, H &amp; Faloutsos, C. found that the tightness of the lower bound for each technique is proportional to the length of gray lines used in the illustrations."},{"type":"heading","depth":4,"payload":{"lines":[387,388]},"content":"Experiments show that after setting the warping window for maximum accuracy for a problem, only 6% of the work needs to be done with LB_Keogh, 0.3% with LB_Keogh, and 0.21% with LB_Yi."},{"type":"heading","depth":4,"payload":{"lines":[388,389]},"content":"As the size of the dataset increases, lower bounds become more important and can prune a larger fraction of the data."},{"type":"heading","depth":4,"payload":{"lines":[389,390]},"content":"From a similarity search/classification point of view, DTW can be linear."}]}]}]},{"type":"heading","depth":1,"payload":{"lines":[404,405]},"content":"Week 9 timeseries 2, lower bounding and advanced topics","children":[{"type":"heading","depth":2,"payload":{"lines":[407,408]},"content":"Time series","children":[{"type":"heading","depth":3,"payload":{"lines":[409,410]},"content":"Evaluating Structural Similarity in Long Time Series","children":[{"type":"heading","depth":4,"payload":{"lines":[410,411]},"content":"Lower bounding is a technique used to speed up similarity search under Dynamic Time Warping (DTW)."},{"type":"heading","depth":4,"payload":{"lines":[411,412]},"content":"It only does the expensive, full calculations when it is absolutely necessary."},{"type":"heading","depth":4,"payload":{"lines":[412,413]},"content":"To consider similarity at the structural level for long time series, we need to measure similarity based on high level structure."},{"type":"heading","depth":4,"payload":{"lines":[413,414]},"content":"Feature-based classification is a way to extract global features from a time series and create a feature vector to measure similarity and/or classify."},{"type":"heading","depth":4,"payload":{"lines":[414,415]},"content":"Features can include mean, variance, skewness, kurtosis, and their respective first derivatives."},{"type":"heading","depth":4,"payload":{"lines":[415,416]},"content":"A multi-layer perceptron neural network can be used as a learning algorithm."},{"type":"heading","depth":4,"payload":{"lines":[416,417]},"content":"Despite this, Euclidean distance was found to give better classification accuracy."}]},{"type":"heading","depth":3,"payload":{"lines":[418,419]},"content":"Compression Based Dissimilarity for Time Series","children":[{"type":"heading","depth":4,"payload":{"lines":[419,420]},"content":"Power (Italian and Dutch): Jan-March, April-June"},{"type":"heading","depth":4,"payload":{"lines":[420,421]},"content":"Balloon1 and Balloon2 (lagged)"},{"type":"heading","depth":4,"payload":{"lines":[421,422]},"content":"Foetal ECG (abdominal and thoracic)"},{"type":"heading","depth":4,"payload":{"lines":[422,423]},"content":"Exchange Rate: Swiss Franc, German Mark"},{"type":"heading","depth":4,"payload":{"lines":[423,424]},"content":"Sunspots: 1749 to 1869, 1869 to 1990"},{"type":"heading","depth":4,"payload":{"lines":[424,425]},"content":"Buoy Sensor: North Salinity, East Salinity"},{"type":"heading","depth":4,"payload":{"lines":[425,426]},"content":"Great Lakes (Erie)"}]},{"type":"heading","depth":3,"payload":{"lines":[427,428]},"content":"Time Series Similarity","children":[{"type":"heading","depth":4,"payload":{"lines":[428,429]},"content":"Use Dynamic Time Warping (DTW) for short time series, and search over the warping window size/shape"},{"type":"heading","depth":4,"payload":{"lines":[429,430]},"content":"Use envelope-based lower bounds to speed up the process"},{"type":"heading","depth":4,"payload":{"lines":[430,431]},"content":"Use compression-based dissimilarity for long time series"},{"type":"heading","depth":4,"payload":{"lines":[431,432]},"content":"Leverage knowledge to extract features"},{"type":"heading","depth":4,"payload":{"lines":[432,433]},"content":"Create an approximation of the data which fits in main memory"},{"type":"heading","depth":4,"payload":{"lines":[433,434]},"content":"Approximately solve the problem at hand in main memory"},{"type":"heading","depth":4,"payload":{"lines":[434,435]},"content":"Make accesses to the original data to confirm/modify the solution"},{"type":"heading","depth":4,"payload":{"lines":[435,436]},"content":"Lower bounding can be used if the approximation allows it"},{"type":"heading","depth":4,"payload":{"lines":[436,437]},"content":"Utilize the Generic Data Mining Algorithm"}]}]},{"type":"heading","depth":2,"payload":{"lines":[439,440]},"content":"Lower Bounding","children":[{"type":"heading","depth":3,"payload":{"lines":[440,441]},"content":"Lower Bounding for Representations","children":[{"type":"heading","depth":4,"payload":{"lines":[441,442]},"content":"Lower bounding for representations is a similar idea to lower bounding for distance measures."},{"type":"heading","depth":4,"payload":{"lines":[442,443]},"content":"It means that for all Query (Q) and Sequence (S), DLB(Q’,S’) ≤ D(Q,S)."},{"type":"heading","depth":4,"payload":{"lines":[443,444]},"content":"Dimensionality reduction is a simple process of converting the time series of size n to another representation of size m, where m &lt;&lt; n."},{"type":"heading","depth":4,"payload":{"lines":[444,445]},"content":"Discrete Fourier Transform (DFT) can be used to decompose the data into 64 pure sine waves."},{"type":"heading","depth":4,"payload":{"lines":[445,446]},"content":"The Fourier Coefficients can be reproduced as a column of numbers."},{"type":"heading","depth":4,"payload":{"lines":[446,447]},"content":"At this stage, we have changed the representation, but not done dimensionality reduction."}]},{"type":"heading","depth":3,"payload":{"lines":[449,450]},"content":"Analysis of Slide 210 Fourier Transform","children":[{"type":"heading","depth":4,"payload":{"lines":[450,451]},"content":"Slide 210 contains a Fourier transform, with the corresponding truncated Fourier coefficients C', which has discarded part of the data."},{"type":"heading","depth":4,"payload":{"lines":[451,452]},"content":"The Fourier coefficients, which follow the truncated coefficients, range from 0.4995 to 0.8407, with the first few sine waves being the largest."},{"type":"heading","depth":4,"payload":{"lines":[452,453]},"content":"The raw data suggests that we can truncate most of the small coefficients with little effect."}]},{"type":"heading","depth":3,"payload":{"lines":[454,455]},"content":"An Example of a Dimensionality Reduction Technique","children":[{"type":"heading","depth":4,"payload":{"lines":[455,456]},"content":"Truncated Fourier Coefficients 1 and 2 were calculated from Raw Data 1 and 2 respectively"},{"type":"heading","depth":4,"payload":{"lines":[456,457]},"content":"Euclidean distance between Truncated Fourier Coefficient vectors is always less than or equal to the Euclidean distance between the two Raw Data vectors"},{"type":"heading","depth":4,"payload":{"lines":[457,458]},"content":"DFT allows for lower bounding of the two data sets"}]},{"type":"heading","depth":3,"payload":{"lines":[459,460]},"content":"An Overview of Parseval's Theorem and the Generic Data Mining Algorithm","children":[{"type":"heading","depth":4,"payload":{"lines":[460,461]},"content":"Parseval's Theorem Slide 23 presents 1.5698, 1.0485, 0.7160, 0.8406, 0.3709, 0.4670, 0.2667, and 0.1928 truncated Fourier coefficients."},{"type":"heading","depth":4,"payload":{"lines":[461,462]},"content":"Raw Data 1 contains 0.4995, 0.5264, 0.5523, 0.5761, 0.5973, 0.6153, 0.6301, 0.6420, 0.6515, 0.6596, 0.6672, 0.6751, 0.6843, 0.6954, 0.7086, 0.7240, 0.7412, 0.7595, 0.7780, and 0.7956."},{"type":"heading","depth":4,"payload":{"lines":[462,463]},"content":"Raw Data 2 consists of 1.1198, 1.4322, 1.0100, 0.4326, 0.5609, 0.8770, 0.1557, and 0.4528 truncated Fourier coefficients."},{"type":"heading","depth":4,"payload":{"lines":[463,464]},"content":"Mini Review for the Generic Data Mining Algorithm includes 0.8115, 0.8247, 0.8345, 0.8407, 0.8431, 0.8423, 0.8387, 0.4995, 0.7412, 0.7595, 0.7780, 0.7956, 0.5264, 0.5523, 0.5761, 0.5973, 0.6153, 0.6301, 0.6420, and 0.6515."},{"type":"heading","depth":4,"payload":{"lines":[464,465]},"content":"Raw Data n has 1.3434, 1.4343, 1.4643, 0.7635, 0.5448, 0.4464, 0.7932, and 0.2126 truncated Fourier coefficients."},{"type":"heading","depth":4,"payload":{"lines":[465,466]},"content":"Keogh, Chakrabarti, Pazzani &amp; Mehrotra (2000) KAIS, Yi &amp; Faloutsos (2000) VLDB, Keogh, Chakrabarti, Pazzani &amp; Mehrotra (2001) SIGMOD, Chan &amp; Fu. (1999) ICDE, Korn, Jagadish &amp; Faloutsos. (1997) SIGMOD, Agrawal, Faloutsos, &amp; Swami. (1993) FODO, Faloutsos, Ranganathan, &amp; Manolopoulos. (1994) SIGMOD, Morinaka, Yoshikawa, Amagasa, &amp; Uemura. (2001) PAKDD, and DFT, DWT, and SVD are mentioned."}]},{"type":"heading","depth":3,"payload":{"lines":[467,468]},"content":"Lower-bounding Time Series Approximations","children":[{"type":"heading","depth":4,"payload":{"lines":[468,469]},"content":"Lower-bounding is a technique used to avoid costly computations, such as complexity or hard-disk access."},{"type":"heading","depth":4,"payload":{"lines":[469,470]},"content":"To use lower-bounding effectively, an approximate representation and suitable distance metric must be defined."},{"type":"heading","depth":4,"payload":{"lines":[470,471]},"content":"Examples of approximate representations include DFT (discrete Fourier transform)."},{"type":"heading","depth":4,"payload":{"lines":[471,472]},"content":"The distance metric used should be chosen based on the type of data being processed."},{"type":"heading","depth":4,"payload":{"lines":[472,473]},"content":"Lin et al. (2003) proposed many time series approximations, as summarized in Slide 25."}]}]},{"type":"heading","depth":2,"payload":{"lines":[476,477]},"content":"Advanced Topics","children":[{"type":"heading","depth":3,"payload":{"lines":[478,479]},"content":"Advanced Topics: Recommender Systems","children":[{"type":"heading","depth":4,"payload":{"lines":[479,480]},"content":"Traditional Recommendation Algorithms mostly used traditional algorithms: Collaborative Filtering, Cluster Models, and Content-based."},{"type":"heading","depth":4,"payload":{"lines":[480,481]},"content":"User Based Collaborative Filtering represents customers as an N-dimensional vector of items and generates recommendations based on the most similar other customers."},{"type":"heading","depth":4,"payload":{"lines":[481,482]},"content":"Cluster Models divide customer base into segments/clusters and uses purchases and ratings of segment customers to make recommendations."},{"type":"heading","depth":4,"payload":{"lines":[482,483]},"content":"Amazon is an example of a personalized online store."},{"type":"heading","depth":4,"payload":{"lines":[483,484]},"content":"Challenges for Recommender Systems include huge amounts of data, required real-time results, and volatile customer data."}]},{"type":"heading","depth":3,"payload":{"lines":[485,486]},"content":"Amazon's Item-to-Item CF &amp; Netflix Movie Recommendation System","children":[{"type":"heading","depth":4,"payload":{"lines":[486,487]},"content":"Amazon's Item-to-Item CF has scalability and performance achieved by creating the similar-items table offline, and has fast runtime for large datasets with high-quality recommendations."},{"type":"heading","depth":4,"payload":{"lines":[487,488]},"content":"Netflix Movie Recommendation System makes recommendations by comparing the watching and searching habits of similar users and offering movies that share characteristics with films rated highly."},{"type":"heading","depth":4,"payload":{"lines":[488,489]},"content":"It uses collaborative, content-based, knowledge-based, and demographic techniques as the basis of its system, and an ensemble method of 107 different algorithmic approaches blended into a single prediction."}]},{"type":"heading","depth":3,"payload":{"lines":[490,491]},"content":"Google YouTube Recommendation System","children":[{"type":"heading","depth":4,"payload":{"lines":[491,492]},"content":"Google YouTube replaced its old recommendation system based on random walk with Amazon’s item-to-item collaborative filtering in 2010."},{"type":"heading","depth":4,"payload":{"lines":[492,493]},"content":"Amazon’s item-to-item collaborative filtering was found to be the best for video recommendation."},{"type":"heading","depth":4,"payload":{"lines":[493,494]},"content":"To recommend elective courses to students, data and collaborative filtering need to be used."}]}]}]},{"type":"heading","depth":1,"payload":{"lines":[500,501]},"content":"Week 10 Revision","children":[{"type":"heading","depth":2,"payload":{"lines":[502,503]},"content":"Lower-Bounding &amp; Distance Measurement for X-ray Images","children":[{"type":"heading","depth":3,"payload":{"lines":[503,504]},"content":"Preprocessing of the X-ray images is needed to represent the data for distance measurements."},{"type":"heading","depth":3,"payload":{"lines":[504,505]},"content":"A suitable proximity measure for this data set would be the Euclidean distance, as it provides an accurate representation of the distance between two images."},{"type":"heading","depth":3,"payload":{"lines":[505,506]},"content":"Lower-bounding can be used to solve the problem of retrieving X-ray images that are closest to a given image, by approximating the actual distance between two images using a lower-bound representation of the data."},{"type":"heading","depth":3,"payload":{"lines":[506,507]},"content":"This can save time by approximating the distance function using the same presentation, or save memory by approximating both the presentation and the distance."},{"type":"heading","depth":3,"payload":{"lines":[507,508]},"content":"The lower-bound function LB(i1,i2) can be used to approximate the actual distance between two images, img1 and img2, using the lower-bound representation i1 and i2, respectively."}]},{"type":"heading","depth":2,"payload":{"lines":[509,510]},"content":"Tracing 1-nearest neighbor Algorithm using Rapidminer","children":[{"type":"heading","depth":3,"payload":{"lines":[510,511]},"content":"Trace the 1-nearest neighbor algorithm using the table provided. The hard drive needs to be accessed 4 times."},{"type":"heading","depth":3,"payload":{"lines":[511,512]},"content":"In order to predict the type of leukemia for patients (ALL or AML), use rapidminer. Connect the decision tree, apply model and classification performance operators as a subprocess of the validation operator."},{"type":"heading","depth":3,"payload":{"lines":[512,513]},"content":"To evaluate the k-nearest neighbor algorithm (K-NN operator), add the K-NN operator to the subprocess."},{"type":"heading","depth":3,"payload":{"lines":[513,514]},"content":"The accuracy of the evaluated algorithm is 0.875."}]}]}]},{"initialExpandLevel":2})</script>
</body>
</html>
