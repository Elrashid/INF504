<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.14.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@6.7.0"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.14.4"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.14.4/dist/index.umd.min.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{"type":"heading","depth":0,"payload":{"lines":[0,1]},"content":"INF504","children":[{"type":"heading","depth":1,"payload":{"lines":[3,4]},"content":"week1-Introduction to Data Mining","children":[{"type":"heading","depth":2,"payload":{"lines":[6,7]},"content":"Introduction to Data Mining course notes","children":[{"type":"heading","depth":3,"payload":{"lines":[7,8]},"content":"The course involves 10 weeks of lectures and incorporates labs, a mini project, critical review, and exams. The mini-project accounts for 40% of the course grade, while the critical review is worth 20%."},{"type":"heading","depth":3,"payload":{"lines":[8,9]},"content":"Course tools include WEKA, Rapid miner, Matlab, Excel and R, with no restriction on tool use as long as students are comfortable with them."},{"type":"heading","depth":3,"payload":{"lines":[9,10]},"content":"Reasons for data mining include commercial and scientific viewpoints. Data mining can be used to better understand large data sets and discover interesting patterns within them."},{"type":"heading","depth":3,"payload":{"lines":[10,11]},"content":"Google N-gram data is used to demonstrate the evolution of language with metrics such as the occurrence of individualistic versus communal words over the years."}]},{"type":"heading","depth":2,"payload":{"lines":[12,13]},"content":"Data Mining Techniques","children":[{"type":"heading","depth":3,"payload":{"lines":[13,14]},"content":"Model Classification for Direct Marketing: Use data for a similar product to target customers likely to buy. Collect demographic and lifestyle information to learn a classifier model."},{"type":"heading","depth":3,"payload":{"lines":[14,15]},"content":"Customer Attrition/Churn Classification: Predict customer loss to competitors using transaction records to find loyalty attributes and create a loyalty model."},{"type":"heading","depth":3,"payload":{"lines":[15,16]},"content":"Classifying Galaxies: Early and Late classes based on formation stages, using data size, an object catalog, and image database with attributes such as light wave characteristics."},{"type":"heading","depth":3,"payload":{"lines":[16,17]},"content":"Clustering for Market Segmentation: Divide a market into distinct customer subsets by collecting geographical and lifestyle attributes to find clusters."},{"type":"heading","depth":3,"payload":{"lines":[17,18]},"content":"Association Rule Discovery for Marketing and Sales Promotion: Use dependency rules to predict item occurrence, such as using bagels to determine products that would be affected if discontinued."}]},{"type":"heading","depth":2,"payload":{"lines":[19,20]},"content":"Overview of SKICAT Data Set and its Analysis","children":[{"type":"heading","depth":3,"payload":{"lines":[20,21]},"content":"The SKICAT Data Set contains image data of roughly 10⁹ objects in the deep sky, with 38 initial features measured about each object."},{"type":"heading","depth":3,"payload":{"lines":[21,22]},"content":"A portion of the objects are labeled by astronomers, and two more features are added in the analysis phase."},{"type":"heading","depth":3,"payload":{"lines":[22,23]},"content":"Four features are normalized to adjust for differences across images."},{"type":"heading","depth":3,"payload":{"lines":[23,24]},"content":"Decision trees are used for analysis, particularly with the Star with Fuzz feature and determining the classification of objects as Star, Galaxy, or Artifact."},{"type":"heading","depth":3,"payload":{"lines":[24,25]},"content":"The search technique for analysis involves a recursive building of the decision tree using several greedy heuristics."},{"type":"heading","depth":3,"payload":{"lines":[25,26]},"content":"The risk of meaningless pattern discovery is discussed, citing Bonferroni's principle and the Rhine Paradox."}]}]},{"type":"heading","depth":1,"payload":{"lines":[31,32]},"content":"week21-31-data-new","children":[{"type":"heading","depth":2,"payload":{"lines":[33,34]},"content":"Data Mining: Types of Attributes and Datasets","children":[{"type":"heading","depth":3,"payload":{"lines":[34,35]},"content":"This week's focus is on types of attributes and datasets in data mining."},{"type":"heading","depth":3,"payload":{"lines":[35,36]},"content":"Data refers to a collection of objects and their attributes that describe them."},{"type":"heading","depth":3,"payload":{"lines":[36,37]},"content":"Attributes are characteristics of objects, such as eye color, temperature, and income."},{"type":"heading","depth":3,"payload":{"lines":[37,38]},"content":"Objects are records that have different attributes, known as variables, fields, or features."},{"type":"heading","depth":3,"payload":{"lines":[38,39]},"content":"There are two types of attributes, discrete and continuous, which have finite or real values, respectively."},{"type":"heading","depth":3,"payload":{"lines":[39,40]},"content":"Moreover, attributes can also be classified as nominal, ordinal, interval, and ratio, depending on their information content."},{"type":"heading","depth":3,"payload":{"lines":[40,41]},"content":"Nominal attributes provide only enough information to distinguish one object from another, while ratio variables allow for meaningful comparisons and operations."}]},{"type":"heading","depth":2,"payload":{"lines":[42,43]},"content":"Techniques for Data Dimensionality Reduction","children":[{"type":"heading","depth":3,"payload":{"lines":[43,44]},"content":"Subset Selection is a technique to reduce dimensionality of data by eliminating redundant and irrelevant features."},{"type":"heading","depth":3,"payload":{"lines":[44,45]},"content":"Feature Subset Selection methods include Brute-force Approaches, Embedded Approaches, Filter Approaches (independent of mining technique) and Wrapper Approaches (using the data mining algorithm to find the best subset of attributes)."},{"type":"heading","depth":3,"payload":{"lines":[45,46]},"content":"Feature Creation can create new attributes more efficiently than the original ones. The methodologies include Feature Extraction, Feature Construction and Mapping Data to New Space."},{"type":"heading","depth":3,"payload":{"lines":[46,47]},"content":"Attribute Transformation can map the entire set of values of an attribute to a new set of replacement values using functions such as xk, log(x), ex or |x|."},{"type":"heading","depth":3,"payload":{"lines":[47,48]},"content":"Standardization and Normalization can scale features to be labeled in the same units or have the same range of values."},{"type":"heading","depth":3,"payload":{"lines":[48,49]},"content":"Netflix, a company with over 100 million data tuples on 17,770 movies and 480,000 users, uses data mining techniques to predict the ratings of users for movies they did not rate, recommending movies based on user/movie/rating data."}]},{"type":"heading","depth":2,"payload":{"lines":[50,51]},"content":"Understanding Similarity and Distance Metrics in Data Analysis","children":[{"type":"heading","depth":3,"payload":{"lines":[51,52]},"content":"The text discusses preprocessing methods for data analysis, including data reduction techniques for large datasets."},{"type":"heading","depth":3,"payload":{"lines":[52,53]},"content":"The types of datasets are also explained, including records, timeseries, and graphs, as well as the different types of attributes, such as numeric and categorical."},{"type":"heading","depth":3,"payload":{"lines":[53,54]},"content":"The concept of similarity and dissimilarity between objects is introduced, with similarity being a numerical measure of how alike two objects are."},{"type":"heading","depth":3,"payload":{"lines":[54,55]},"content":"Proximity is defined as a measure of similarity or dissimilarity, and the text provides an example of measuring employee similarity based on their attributes."},{"type":"heading","depth":3,"payload":{"lines":[55,56]},"content":"Simple and complex attribute similarities and dissimilarities are discussed, with examples including patient diagnosis and X-rays."},{"type":"heading","depth":3,"payload":{"lines":[56,57]},"content":"Two distance metrics are described: the Euclidean Distance, and its generalization, the Minkowski Distance."},{"type":"heading","depth":3,"payload":{"lines":[57,58]},"content":"Standardization is highlighted as one instance when the preprocessing of data may be necessary."}]},{"type":"heading","depth":2,"payload":{"lines":[59,60]},"content":"Understanding Distance and Similarity Metrics","children":[{"type":"heading","depth":3,"payload":{"lines":[60,61]},"content":"Euclidean distance is a measure of distance between vectors in n-dimensional space."},{"type":"heading","depth":3,"payload":{"lines":[61,62]},"content":"Minkowski Distance takes different forms based on the chosen value of parameter r."},{"type":"heading","depth":3,"payload":{"lines":[62,63]},"content":"Distance matrix stores the distance (dissimilarity) between all possible combinations of points (data objects)."},{"type":"heading","depth":3,"payload":{"lines":[63,64]},"content":"A distance should be positive definite, symmetric, and satisfy the triangle inequality to be a metric."},{"type":"heading","depth":3,"payload":{"lines":[64,65]},"content":"Similarities have properties such as symmetry and being equal to 1 when two objects are identical."},{"type":"heading","depth":3,"payload":{"lines":[65,66]},"content":"Binary vectors can be compared using Simple Matching and Jaccard Coefficients."},{"type":"heading","depth":3,"payload":{"lines":[66,67]},"content":"Cosine similarity measures the similarity of two vectors in terms of the cosine of the angle between them."}]}]},{"type":"heading","depth":1,"payload":{"lines":[72,73]},"content":"week41_data_exploration","children":[{"type":"heading","depth":2,"payload":{"lines":[74,75]},"content":"Exploring DataToday Visualization techniques Lab","children":[{"type":"heading","depth":3,"payload":{"lines":[75,76]},"content":"Data exploration is the preliminary exploration of data to understand its characteristics such as pattern recognition. The key motivations of data exploration are to help in selecting the right tool for preprocessing or analysis, and to make use of humans' abilities to recognize patterns."},{"type":"heading","depth":3,"payload":{"lines":[76,77]},"content":"Techniques used in EDA, as defined by John Tukey, focused on visualization, clustering, and anomaly detection in data exploration."},{"type":"heading","depth":3,"payload":{"lines":[77,78]},"content":"Summary statistics is a method of summarizing the properties of data through frequency, location, and spread. Examples are mean and standard deviation."},{"type":"heading","depth":3,"payload":{"lines":[78,79]},"content":"Percentiles are used for continuous data and measure the value of x to be less than p% of the observed values."},{"type":"heading","depth":3,"payload":{"lines":[79,80]},"content":"Measures of location (mean and median) and spread (range and variance) are used to determine the position and spread of data. Trimmed mean and median absolute deviation are also used."},{"type":"heading","depth":3,"payload":{"lines":[80,81]},"content":"Visualization is an essential tool in data exploration. Using the example of representing diseases in different emirates, data visualization can help to understand and analyze data better."}]},{"type":"heading","depth":2,"payload":{"lines":[82,83]},"content":"Deaths in Abu Dhabi and Visualization of Historic Data","children":[{"type":"heading","depth":3,"payload":{"lines":[83,84]},"content":"Data contains cause, nationality, and sex of deaths in Abu Dhabi"},{"type":"heading","depth":3,"payload":{"lines":[84,85]},"content":"Visualization of deaths in French army in 1800s invasion of Russia"},{"type":"heading","depth":3,"payload":{"lines":[85,86]},"content":"Figurative map shows successive losses in army numbers, line thickness indicates army numbers, color indicates arrival/leaving of Moscow, X, Y indicates rough location, temperature added"},{"type":"heading","depth":3,"payload":{"lines":[86,87]},"content":"Information extracted from works of various authors and diary of Jacob, pharmacist of the Army"},{"type":"heading","depth":3,"payload":{"lines":[87,88]},"content":"Computers cannot design great visualizations, but can help in speed"},{"type":"heading","depth":3,"payload":{"lines":[88,89]},"content":"Visualization defined and motivated as augmenting human capabilities rather than replacing them"},{"type":"heading","depth":3,"payload":{"lines":[89,90]},"content":"Human in the loop for exploratory tasks, presentation of known results, refining and trust building, and verifying automation."}]},{"type":"heading","depth":2,"payload":{"lines":[91,92]},"content":"The Importance of Human Judgment and Automatic Computation in Data Analysis","children":[{"type":"heading","depth":3,"payload":{"lines":[92,93]},"content":"Differentiate between tasks that require human intervention and those suitable for automation in trading, gene mutation, and elevator control."},{"type":"heading","depth":3,"payload":{"lines":[93,94]},"content":"Learn why modern data visualization must depend on the human visual system and the limitations of sound and touch/haptics."},{"type":"heading","depth":3,"payload":{"lines":[94,95]},"content":"Understand why representing data in detail, as evidenced by Anscombe's Quartet, is vital to discerning patterns and confirming validity."},{"type":"heading","depth":3,"payload":{"lines":[95,96]},"content":"Explore the resource limitations of computational power and human attention and memory when assessing data."},{"type":"heading","depth":3,"payload":{"lines":[96,97]},"content":"Define the concepts of marks and channels in data visualization and how they work together to create a visual language capable of encoding complex information."}]},{"type":"heading","depth":2,"payload":{"lines":[98,99]},"content":"Best Practices for Graphical Data Presentation","children":[{"type":"heading","depth":3,"payload":{"lines":[99,100]},"content":"Use screen share software to present live content and receive responses through polls at pollev.com"},{"type":"heading","depth":3,"payload":{"lines":[100,101]},"content":"Choose between bar and line charts based on the type of key attribute – bar charts for categorical, line charts for ordered attributes"},{"type":"heading","depth":3,"payload":{"lines":[101,102]},"content":"Avoid line charts for categorical attributes as the implication of trend would override semantics"},{"type":"heading","depth":3,"payload":{"lines":[102,103]},"content":"Labelled axes and including 0 at the bottom left are critical for preventing cropping and misleading slopes"},{"type":"heading","depth":3,"payload":{"lines":[103,104]},"content":"Dual-axis line charts are controversial and should only be used with commensurate attribs"},{"type":"heading","depth":3,"payload":{"lines":[104,105]},"content":"Slopegraphs are effective for comparing changes in rank/value over time or among groups"},{"type":"heading","depth":3,"payload":{"lines":[105,106]},"content":"Connected scatterplots are popular in journalism as an alternative to dual-axis charts for showing temporal trends with value attribs on both axes."}]},{"type":"heading","depth":2,"payload":{"lines":[107,108]},"content":"Evolution of Auto Safety Features","children":[{"type":"heading","depth":3,"payload":{"lines":[108,109]},"content":"After the energy crises of the 1970s, President Nixon sets a 55 mph speed limit as a national energy policy."},{"type":"heading","depth":3,"payload":{"lines":[109,110]},"content":"Curtailed fuel supplies from the Iranian Revolution and Iran-Iraq war in the 1980s lead to decreased driving and fatalities."},{"type":"heading","depth":3,"payload":{"lines":[110,111]},"content":"In 1984, the first mandatory seat belt law is introduced, followed by child car seat laws and DUI regulations."},{"type":"heading","depth":3,"payload":{"lines":[111,112]},"content":"By the 1990s, computerized auto technology like antilock brakes is reducing accidents."},{"type":"heading","depth":3,"payload":{"lines":[112,113]},"content":"The empirical correlation between air bags and SUVs remains unclear."},{"type":"heading","depth":3,"payload":{"lines":[113,114]},"content":"Gantt charts use one key and two related values to emphasize temporal overlaps and dependencies in project scheduling."},{"type":"heading","depth":3,"payload":{"lines":[114,115]},"content":"Scatterplot matrices and parallel coordinates are idioms used for data visualization in statistics."}]},{"type":"heading","depth":2,"payload":{"lines":[116,117]},"content":"Understanding Advanced Data Visualization Techniques","children":[{"type":"heading","depth":3,"payload":{"lines":[117,118]},"content":"Axes and point mark: All possible pairs of axes, scalability in terms of dozens to hundreds of items in Math, Physics, Dance, and Drama."},{"type":"heading","depth":3,"payload":{"lines":[118,119]},"content":"Parallel coordinates: Axes ordering is a major challenge. Jagged lines represent items in parallel axes in Math, while Dance and Physics have continuous parallel lines."},{"type":"heading","depth":3,"payload":{"lines":[119,120]},"content":"Scatterplot Matrix: Positive correlation diagonal low-to-high, negative correlation diagonal high-to-low, and uncorrelated is spread out."},{"type":"heading","depth":3,"payload":{"lines":[120,121]},"content":"Task: Correlation scattered crossings."},{"type":"heading","depth":3,"payload":{"lines":[121,122]},"content":"Layered Grammar of Graphics: Visualization of price and carat using parallel coordinates with positive and negative correlation."},{"type":"heading","depth":3,"payload":{"lines":[122,123]},"content":"Hyper-dimensional data analysis using parallel coordinates: Visualization of six-dimensional data."},{"type":"heading","depth":3,"payload":{"lines":[123,124]},"content":"Group Exercise: Visualization of data on Covid-19 patients' recovery date of infection and recovery, and bus ridership from station to station for 2020, and 2020 vs. 2019."},{"type":"heading","depth":3,"payload":{"lines":[124,125]},"content":"Domain-specific and specialized tools: Techniques such as label, legend, range render, tree builder, scale binding, circle layout, encoder, node sprite, data sprite, data tree visualization, label legend data, tooltip control, selection control, operator controls, sunburst flare, physics query scale, transitioner, interpolate converters, and Dirty Sprite Text Sprite."},{"type":"heading","depth":3,"payload":{"lines":[125,126]},"content":"NBodyForce Simulation: Query methods, node-link diagrams, interpolator, and GraphML converters used in advanced data visualization."}]}]},{"type":"heading","depth":1,"payload":{"lines":[133,134]},"content":"week51_classification","children":[{"type":"heading","depth":2,"payload":{"lines":[135,136]},"content":"Title: Decision Tree Induction Algorithm for Classification Task","children":[{"type":"heading","depth":3,"payload":{"lines":[136,137]},"content":"The text describes Hunt's algorithm, CART, ID3, C4.5, SLIQ, and SPRINT as decision tree induction algorithms for classification tasks."},{"type":"heading","depth":3,"payload":{"lines":[137,138]},"content":"Hunt's algorithm has a general structure where training records are split into smaller subsets based on attribute tests until each subset belongs to one class."},{"type":"heading","depth":3,"payload":{"lines":[138,139]},"content":"The algorithm then labels the leaf nodes based on the majority of the class."},{"type":"heading","depth":3,"payload":{"lines":[139,140]},"content":"The text provides training and test data with attributes such as Refund, Marital Status, Taxable Income, and Cheat labeled as Yes or No."},{"type":"heading","depth":3,"payload":{"lines":[140,141]},"content":"The goal is to apply the decision tree model to the test data and classify Cheat as Yes or No based on attributes Refund, Marital Status, and Taxable Income."}]},{"type":"heading","depth":2,"payload":{"lines":[142,143]},"content":"Learning About Nearest-Neighbor Classifiers and Artificial Neural Networks","children":[{"type":"heading","depth":3,"payload":{"lines":[143,144]},"content":"Nearest-neighbor classifiers require a set of stored records, a similarity/dissimilarity measure, and a value for k to retrieve k nearest neighbors to classify an unknown record."},{"type":"heading","depth":3,"payload":{"lines":[144,145]},"content":"The nearest neighbors of a record are data points with the k smallest distance to that record."},{"type":"heading","depth":3,"payload":{"lines":[145,146]},"content":"To determine the class label of an unknown record, compute similarity to other training records, identify k nearest neighbors, and use class labels of the nearest neighbors (e.g., by taking majority vote)."},{"type":"heading","depth":3,"payload":{"lines":[146,147]},"content":"Choosing the value of k is crucial, as selecting a value that is too small or too large can result in sensitivity to noise points or neighborhood points from other classes, respectively."},{"type":"heading","depth":3,"payload":{"lines":[147,148]},"content":"Scaling issues may also arise due to varying attribute values, requiring them to be scaled to prevent distance measures from being dominated by one attribute."},{"type":"heading","depth":3,"payload":{"lines":[148,149]},"content":"The curse of dimensionality can lead to counter-intuitive results for high dimensional data, but nearest-neighbor classifiers and scaling may solve the problem."},{"type":"heading","depth":3,"payload":{"lines":[149,150]},"content":"Artificial Neural Networks (ANN) are an assembly of inter-connected nodes and weighted links that can output a class label based on input values and weights."}]},{"type":"heading","depth":2,"payload":{"lines":[151,152]},"content":"Methods for Performance Evaluation","children":[{"type":"heading","depth":3,"payload":{"lines":[152,153]},"content":"Reliable estimation methods for performance evaluation are necessary."},{"type":"heading","depth":3,"payload":{"lines":[153,154]},"content":"Metrics should focus on the predictive capability of a model."},{"type":"heading","depth":3,"payload":{"lines":[154,155]},"content":"The confusion matrix is a useful tool for evaluation, with four categories: true positive, false negative, false positive, and true negative."},{"type":"heading","depth":3,"payload":{"lines":[155,156]},"content":"The most widely-used metric for evaluation is accuracy, which is calculated by dividing the sum of true positive and true negative by the total population."},{"type":"heading","depth":3,"payload":{"lines":[156,157]},"content":"The accuracy metric can lead to misleading results and may require stratified sampling."},{"type":"heading","depth":3,"payload":{"lines":[157,158]},"content":"Cost-sensitive measures like precision, recall, and F-measure can also be used for evaluation."},{"type":"heading","depth":3,"payload":{"lines":[158,159]},"content":"There are several methods for obtaining reliable estimates, including holdout and cross-validation."},{"type":"heading","depth":3,"payload":{"lines":[159,160]},"content":"Leave-one-out cross-validation is a version where k is equal to the size of the data set."}]}]},{"type":"heading","depth":1,"payload":{"lines":[166,167]},"content":"week_61_cluster_analysis","children":[{"type":"heading","depth":2,"payload":{"lines":[168,169]},"content":"Cluster Analysis Basic Concepts and Algorithms","children":[{"type":"heading","depth":3,"payload":{"lines":[169,170]},"content":"Cluster Analysis is finding groups of objects that are similar to each other and different from objects in other groups."},{"type":"heading","depth":3,"payload":{"lines":[170,171]},"content":"The type of proximity or density measure used to decide the distance/similarity metric is crucial."},{"type":"heading","depth":3,"payload":{"lines":[171,172]},"content":"There are two types of clustering: partitional and hierarchical."},{"type":"heading","depth":3,"payload":{"lines":[172,173]},"content":"K-means is a popular partitional clustering approach where each cluster is associated with a centroid and each point is assigned to the cluster with the closest centroid."},{"type":"heading","depth":3,"payload":{"lines":[173,174]},"content":"Convergence usually happens in the first few iterations of K-means clustering."},{"type":"heading","depth":3,"payload":{"lines":[174,175]},"content":"The initial centroids in K-means clustering are chosen randomly, making it non-optimal."},{"type":"heading","depth":3,"payload":{"lines":[175,176]},"content":"Selecting initial points in K-means clustering can be problematic, especially when dealing with a large number of clusters."}]},{"type":"heading","depth":2,"payload":{"lines":[177,178]},"content":"Understanding and Overcoming Limitations of K-means Clustering Algorithm","children":[{"type":"heading","depth":3,"payload":{"lines":[178,179]},"content":"Different initial centroids can result in different clusterings in K-means."},{"type":"heading","depth":3,"payload":{"lines":[179,180]},"content":"Sum of Squared Error (SSE) is a common measure used to evaluate different clusterings."},{"type":"heading","depth":3,"payload":{"lines":[180,181]},"content":"Increasing K can reduce SSE, but good clustering with smaller K can have lower SSE than poor clustering with higher K."},{"type":"heading","depth":3,"payload":{"lines":[181,182]},"content":"Pre-processing and post-processing techniques like normalizing data, eliminating outliers, splitting and merging clusters can improve clustering results."},{"type":"heading","depth":3,"payload":{"lines":[182,183]},"content":"Bisecting K-means is a variant of K-means that can produce a partitional or hierarchical clustering."},{"type":"heading","depth":3,"payload":{"lines":[183,184]},"content":"Limitations of K-means include difficulties with clusters of differing sizes, densities, and non-globular shapes."},{"type":"heading","depth":3,"payload":{"lines":[184,185]},"content":"One solution to overcome limitations is to use many clusters and find parts of clusters."}]},{"type":"heading","depth":2,"payload":{"lines":[186,187]},"content":"Introduction to Clustering Algorithms and K-means","children":[{"type":"heading","depth":3,"payload":{"lines":[187,188]},"content":"Clustering vs. Classification: Briefly explains the difference between clustering and classification algorithms."},{"type":"heading","depth":3,"payload":{"lines":[188,189]},"content":"K-means Clusters using Euclidean Distance: Explains how to create two clusters using K-means with the given dataset and initial centroids."},{"type":"heading","depth":3,"payload":{"lines":[189,190]},"content":"Strengths and Types of Hierarchical Clustering: Explains the advantages and two main types of Hierarchical clustering, agglomerative and divisive."},{"type":"heading","depth":3,"payload":{"lines":[190,191]},"content":"Agglomerative Clustering Algorithm: Describes the step-by-step process of agglomerative clustering, merging closest clusters until only one or k clusters are left."},{"type":"heading","depth":3,"payload":{"lines":[191,192]},"content":"Defining Inter-Cluster Similarity: Discusses the different approaches for defining inter-cluster similarity, including the use of distance or similarity matrices."}]},{"type":"heading","depth":2,"payload":{"lines":[193,194]},"content":"Clustering Software Development","children":[{"type":"heading","depth":3,"payload":{"lines":[194,195]},"content":"O(N³) time in many cases for clustering software development"},{"type":"heading","depth":3,"payload":{"lines":[195,196]},"content":"Hierarchical clustering poses problems such as not being able to undo a decision to combine two clusters, and different schemes having problems with noise, outliers, convex shapes, and different sized clusters"},{"type":"heading","depth":3,"payload":{"lines":[196,197]},"content":"K-means, hierarchical clustering, and density-based clustering (DBSCAN) are all clustering algorithms with DBSCAN being a density-based algorithm"},{"type":"heading","depth":3,"payload":{"lines":[197,198]},"content":"DBSCAN works well with varying densities and clusters of different shapes and sizes, but not with high-dimensional data"},{"type":"heading","depth":3,"payload":{"lines":[198,199]},"content":"Cluster validity can be determined through different aspects such as clustering tendency, external comparison to known results, or internal comparison within the resulting clusters"}]},{"type":"heading","depth":2,"payload":{"lines":[200,201]},"content":"Measuring Cluster Validity Via Correlation and Similarity Matrix","children":[{"type":"heading","depth":3,"payload":{"lines":[201,202]},"content":"Evaluating the fit of cluster analysis without external information."},{"type":"heading","depth":3,"payload":{"lines":[202,203]},"content":"Comparing the validity of two different cluster analyses to determine the better one."},{"type":"heading","depth":3,"payload":{"lines":[203,204]},"content":"Determining the correct number of clusters, evaluating the entire clustering or individual clusters."},{"type":"heading","depth":3,"payload":{"lines":[204,205]},"content":"Different types of numerical measures to judge cluster validity, including External, Internal, and Relative Index."},{"type":"heading","depth":3,"payload":{"lines":[205,206]},"content":"Measuring cluster validity via correlation using proximity and incidence matrices."},{"type":"heading","depth":3,"payload":{"lines":[206,207]},"content":"Inspecting visually ordered similarity matrix with respect to cluster labels for validation."},{"type":"heading","depth":3,"payload":{"lines":[207,208]},"content":"Clusters in random data may not be crisp."},{"type":"heading","depth":3,"payload":{"lines":[208,209]},"content":"Using similarity matrix for cluster validation in DBSCAN."}]},{"type":"heading","depth":2,"payload":{"lines":[210,211]},"content":"Cluster Validation with Similarity Matrix","children":[{"type":"heading","depth":3,"payload":{"lines":[211,212]},"content":"Clusters in random data are unclear and not well separated."},{"type":"heading","depth":3,"payload":{"lines":[212,213]},"content":"DBSCAN is not a good measure for density or contiguity based clusters."},{"type":"heading","depth":3,"payload":{"lines":[213,214]},"content":"Internal Index measures the goodness of a clustering structure without external information."},{"type":"heading","depth":3,"payload":{"lines":[214,215]},"content":"SSE (Sum of Squares Error) can be used to estimate the number of clusters or compare two clusterings."},{"type":"heading","depth":3,"payload":{"lines":[215,216]},"content":"Frameworks, such as statistical analysis, provide a way to interpret validation measures."},{"type":"heading","depth":3,"payload":{"lines":[216,217]},"content":"Atypical clustering results are more likely to represent valid structure in the data."},{"type":"heading","depth":3,"payload":{"lines":[217,218]},"content":"Comparing index values from random data or clusterings with cluster results can determine validity."},{"type":"heading","depth":3,"payload":{"lines":[218,219]},"content":"Cluster validation is the most difficult and frustrating part of cluster analysis."}]}]},{"type":"heading","depth":1,"payload":{"lines":[225,226]},"content":"week71_Retrieval by Content_1","children":[{"type":"heading","depth":2,"payload":{"lines":[227,228]},"content":"Text Retrieval Techniques","children":[{"type":"heading","depth":3,"payload":{"lines":[228,229]},"content":"Retrieval aims to find stored data efficiently using associated query language."},{"type":"heading","depth":3,"payload":{"lines":[229,230]},"content":"Exact queries limit finding similar data objects required for comparisons."},{"type":"heading","depth":3,"payload":{"lines":[230,231]},"content":"Complex data objects require techniques for representation and similarity measurement."},{"type":"heading","depth":3,"payload":{"lines":[231,232]},"content":"Text retrieval focuses on retrieving relevant documents using NLP and bag-of-words techniques."},{"type":"heading","depth":3,"payload":{"lines":[232,233]},"content":"Vector space representation gains popularity to retain as much content as possible."},{"type":"heading","depth":3,"payload":{"lines":[233,234]},"content":"Terms related to a database and regression can represent data objects."},{"type":"heading","depth":3,"payload":{"lines":[234,235]},"content":"Precision and recall are used for measuring retrieval performance."},{"type":"heading","depth":3,"payload":{"lines":[235,236]},"content":"Retrieval can be viewed as classification with true/false positive/negative sequences used in a confusion matrix."}]},{"type":"heading","depth":2,"payload":{"lines":[237,238]},"content":"Classical Text Retrieval and TF-IDF","children":[{"type":"heading","depth":3,"payload":{"lines":[238,239]},"content":"The document-term matrix containing frequencies of terms in documents is used for text retrieval."},{"type":"heading","depth":3,"payload":{"lines":[239,240]},"content":"Query representation is similar to document representation, and a similarity/distance metric is required to compare them."},{"type":"heading","depth":3,"payload":{"lines":[240,241]},"content":"Euclidean and cosine metrics are compared for distance calculation between documents. Cosine is more widely used as it compares relative frequencies."},{"type":"heading","depth":3,"payload":{"lines":[241,242]},"content":"Not all terms are equally important, and IDF is used to weight them inversely proportional to their frequency."},{"type":"heading","depth":3,"payload":{"lines":[242,243]},"content":"Finally, TF-IDF is used to create a weighted document-term matrix for text retrieval."}]},{"type":"heading","depth":2,"payload":{"lines":[244,245]},"content":"Advanced Text Representation Techniques","children":[{"type":"heading","depth":3,"payload":{"lines":[245,246]},"content":"Advanced Text Representation Techniques: This text discusses two different techniques for representing words in text data, namely Word Representations and Word Embeddings."},{"type":"heading","depth":3,"payload":{"lines":[246,247]},"content":"Word Representations: This traditional method utilizes a Bag of Words model, where each word in the vocabulary is represented using a one-hot encoding technique. The context information is not utilized in this method."},{"type":"heading","depth":3,"payload":{"lines":[247,248]},"content":"Word Embeddings: This unsupervised method represents each word as a vector in a fixed number of dimensions (generally 300). These dimensions are basically projections along different axes. Similar words have similar vectors, making it easier to identify relationships between words."},{"type":"heading","depth":3,"payload":{"lines":[248,249]},"content":"Known Techniques: There are two popular techniques for Word Embeddings, namely Word2Vec and GloVe. Public general embeddings can also be used."},{"type":"heading","depth":3,"payload":{"lines":[249,250]},"content":"Conclusion: Advanced text representation techniques can greatly improve the accuracy and effectiveness of text retrieval."}]}]},{"type":"heading","depth":1,"payload":{"lines":[256,257]},"content":"Week72_Finding Patterns and Rules_new_1","children":[{"type":"heading","depth":2,"payload":{"lines":[258,259]},"content":"Finding Patterns and Rules in Data Mining","children":[{"type":"heading","depth":3,"payload":{"lines":[259,260]},"content":"In the previous lecture, clustering techniques such as K-means, density-based, and hierarchical were introduced."},{"type":"heading","depth":3,"payload":{"lines":[260,261]},"content":"Today's lecture focuses on discovering association rules in data mining."},{"type":"heading","depth":3,"payload":{"lines":[261,262]},"content":"The task is to recommend movies to users using techniques such as classification and clustering."},{"type":"heading","depth":3,"payload":{"lines":[262,263]},"content":"A pattern is an interesting information about part of the data, different from a model that applies to the whole data."},{"type":"heading","depth":3,"payload":{"lines":[263,264]},"content":"Primitive patterns are constraints over variable values, while more complex patterns consist of primitive patterns combined using AND and OR."},{"type":"heading","depth":3,"payload":{"lines":[264,265]},"content":"Conjunctive patterns with binary variables, called itemsets, are used to simplify data mining."},{"type":"heading","depth":3,"payload":{"lines":[265,266]},"content":"Market basket patterns can be applied in real market analysis, mining text, and genetics."},{"type":"heading","depth":3,"payload":{"lines":[266,267]},"content":"Many-many relationships between items and baskets can be modeled as &quot;Market Baskets.&quot;"},{"type":"heading","depth":3,"payload":{"lines":[267,268]},"content":"The scale of the problem requires optimization of the disk access."}]},{"type":"heading","depth":2,"payload":{"lines":[269,270]},"content":"Mining Frequent Itemsets and Association Rules","children":[{"type":"heading","depth":3,"payload":{"lines":[270,271]},"content":"The objective is to find sets of items that appear &quot;frequently&quot; in baskets with a certain support threshold."},{"type":"heading","depth":3,"payload":{"lines":[271,272]},"content":"Frequency of an itemset I is the number of records where I is true, and support is the frequency divided by the total number of baskets/records."},{"type":"heading","depth":3,"payload":{"lines":[272,273]},"content":"The sets of items that appear in ≥ s baskets are called frequent itemsets."},{"type":"heading","depth":3,"payload":{"lines":[273,274]},"content":"Association rules involve if-then statements about the contents of baskets, and the confidence of a rule is the probability of the consequent given the antecedent."},{"type":"heading","depth":3,"payload":{"lines":[274,275]},"content":"The hard part is finding the high-support frequent itemsets, which requires reading data in passes and minimizing disk I/O's."}]}]},{"type":"heading","depth":1,"payload":{"lines":[283,284]},"content":"week81_graph mining intro","children":[{"type":"heading","depth":2,"payload":{"lines":[285,286]},"content":"The Importance of Data Mining Graphs and Networks","children":[{"type":"heading","depth":3,"payload":{"lines":[286,287]},"content":"Graph/network mining is crucial in understanding complex relationships and patterns."},{"type":"heading","depth":3,"payload":{"lines":[287,288]},"content":"Features, such as physical and cyber connections, food webs in ecology, phone call patterns, and word co-occurrence in text, can be analyzed for data mining."},{"type":"heading","depth":3,"payload":{"lines":[288,289]},"content":"Social networks, such as Facebook, LinkedIn, and Twitter, are often used as datasets for graph mining research."},{"type":"heading","depth":3,"payload":{"lines":[289,290]},"content":"Society can be represented as a graph, and relationships can be analyzed through mathematical graph theory."},{"type":"heading","depth":3,"payload":{"lines":[290,291]},"content":"Tasks such as visualization, frequent pattern discovery, and descriptive/generative modeling can be applied to graphs for data mining."},{"type":"heading","depth":3,"payload":{"lines":[291,292]},"content":"Graph mining can be used to find frequent subgraphs, detect communities, and develop simulators."}]},{"type":"heading","depth":2,"payload":{"lines":[293,294]},"content":"Network/graph properties and Graph mining","children":[{"type":"heading","depth":3,"payload":{"lines":[294,295]},"content":"A graph G=(V,E) can be undirected, directed, or weighted. The weight on a graph can represent different things."},{"type":"heading","depth":3,"payload":{"lines":[295,296]},"content":"Vertex degree is the number of edges adjacent to a vertex, while indegree and outdegree are for directed graphs."},{"type":"heading","depth":3,"payload":{"lines":[296,297]},"content":"Degree distribution is a histogram of nodes with different degrees."},{"type":"heading","depth":3,"payload":{"lines":[297,298]},"content":"The figure shows that many websites are not accessible from other websites, and the number of pages corresponds to the total and remote-only indegrees."},{"type":"heading","depth":3,"payload":{"lines":[298,299]},"content":"A graph's diameter is the longest shortest path."},{"type":"heading","depth":3,"payload":{"lines":[299,300]},"content":"Frequent subgraph mining is useful for various fields, but generating candidates and recognizing isomorphism are challenges."}]}]},{"type":"heading","depth":1,"payload":{"lines":[309,310]},"content":"Week82_Time Series Data Mining","children":[{"type":"heading","depth":2,"payload":{"lines":[312,313]},"content":"Introduction to Time Series Data Mining","children":[{"type":"heading","depth":3,"payload":{"lines":[313,314]},"content":"The lecture covers Time Series Data Mining, provided by Eamonn Keogh."},{"type":"heading","depth":3,"payload":{"lines":[314,315]},"content":"There will be a revision session before the exam on March 15, 2016, and no lecture next week."},{"type":"heading","depth":3,"payload":{"lines":[315,316]},"content":"The submission deadline of Assignment 2 is March 28, 2016, and presentations will be on March 29, 2016."},{"type":"heading","depth":3,"payload":{"lines":[316,317]},"content":"Further feedback on Assignment 1 can be expected after class."},{"type":"heading","depth":3,"payload":{"lines":[317,318]},"content":"Time Series are collections of observations made sequentially in time."},{"type":"heading","depth":3,"payload":{"lines":[318,319]},"content":"Time Series can be found in various domains, such as medical, scientific, and business."},{"type":"heading","depth":3,"payload":{"lines":[319,320]},"content":"Challenges exist when working with large databases and subjective similarity matching."},{"type":"heading","depth":3,"payload":{"lines":[320,321]},"content":"To analyze time series data, we need to consider similarity/distance metrics, such as Euclidian and Dynamic Time Warping."}]},{"type":"heading","depth":2,"payload":{"lines":[322,323]},"content":"Preprocessing Techniques for Distance Calculations in Data Mining","children":[{"type":"heading","depth":3,"payload":{"lines":[323,324]},"content":"About 80% of published work in data mining uses Euclidean distance for distance calculations."},{"type":"heading","depth":3,"payload":{"lines":[324,325]},"content":"Preprocessing the data before distance calculations is recommended to obtain meaningful results."},{"type":"heading","depth":3,"payload":{"lines":[325,326]},"content":"Euclidean distance is sensitive to distortions in the data like offset translation, amplitude scaling, and noise transformation."},{"type":"heading","depth":3,"payload":{"lines":[326,327]},"content":"Offset Translation can be removed by subtracting the mean values of the time series."},{"type":"heading","depth":3,"payload":{"lines":[327,328]},"content":"Amplitude Scaling can be removed by dividing the time series by their standard deviation."},{"type":"heading","depth":3,"payload":{"lines":[328,329]},"content":"Noise Transformation can be removed by smoothing the time series."},{"type":"heading","depth":3,"payload":{"lines":[329,330]},"content":"Preprocessing techniques like these make Euclidean distance work better."},{"type":"heading","depth":3,"payload":{"lines":[330,331]},"content":"Dynamic Time Warping is an alternative to Euclidean distance that allows non-linear alignments between sequences."}]},{"type":"heading","depth":2,"payload":{"lines":[332,333]},"content":"Understanding Dynamic Time Warping","children":[{"type":"heading","depth":3,"payload":{"lines":[333,334]},"content":"DTW is a technique that calculates the similarity between two time series."},{"type":"heading","depth":3,"payload":{"lines":[334,335]},"content":"It is more accurate than Euclidean distance, but also much slower to calculate."},{"type":"heading","depth":3,"payload":{"lines":[335,336]},"content":"DTW is calculated by creating a matrix of distances between every pair of points in the two time series."},{"type":"heading","depth":3,"payload":{"lines":[336,337]},"content":"Finding the best path through this matrix gives us the minimum cost of warping."},{"type":"heading","depth":3,"payload":{"lines":[337,338]},"content":"Slightly speeding up the calculations is possible through global constraints such as Sakoe-Chiba Band and Itakura Parallelogram."},{"type":"heading","depth":3,"payload":{"lines":[338,339]},"content":"Lowerbounding can be used to speed up the entire process, making it possible to deal with massive time series under DTW."}]}]}]},{})</script>
</body>
</html>
